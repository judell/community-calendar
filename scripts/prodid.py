#!/usr/bin/env python3
"""
Scan ICS files across all cities and report distinct PRODID values (ICS platforms).
Updates docs/prodid.md with a table of platforms, descriptions, and which cities use them.

Unrecognized PRODIDs are listed in an "Unclassified" section so new platforms
don't silently disappear. To fix: add a pattern to PLATFORM_MAP or OUR_SCRAPERS.
"""

import re
import sys
from pathlib import Path
from collections import defaultdict

# Map raw PRODID patterns to normalized platform names and descriptions.
# Order matters: first match wins.
PLATFORM_MAP = [
    (r'ECPv\d', 'The Events Calendar (WordPress)', 'WordPress plugin (aka "Tribe Events"). PRODID includes site name + ECP version.'),
    (r'wp-events-plugin\.com', 'Modern Events Calendar (WordPress)', 'WordPress plugin by Webnus. PRODID references wp-events-plugin.com.'),
    (r'Meetup//Meetup Calendar', 'Meetup', 'ICS export from meetup.com groups.'),
    (r'tockify\.com', 'Tockify', 'Embeddable calendar widget with public ICS feeds.'),
    (r'Google Inc//Google Calendar', 'Google Calendar', 'Public Google Calendar ICS export.'),
    (r'iCalendar-Ruby', 'Localist', 'Campus/community event platform by Concept3D (uses iCalendar-Ruby gem). Common at universities.'),
    (r'Springshare//LibCal', 'LibCal (Springshare)', 'Library event management platform.'),
    (r'rianjs/ical\.net', 'CivicPlus (ical.net)', 'Government CMS using ical.net library. Common for city/county websites.'),
    (r'BedeWork', 'Bedework', 'Open-source enterprise calendar (Java). Used by Duke and some universities.'),
    (r'MembershipWorks', 'MembershipWorks', 'Membership management platform with event calendar.'),
    (r'growthzone\.com', 'GrowthZone', 'Chamber of commerce / association management platform.'),
    (r'Indiana University', 'Localist', 'Campus/community event platform by Concept3D (Indiana University instance).'),
    (r'Golang ICS Library', 'Golang ICS', 'Events generated by Go ical library.'),
    (r'maxpreps\.com', 'MaxPreps', 'High school sports schedules.'),
    (r'bibliocommons\.com', 'BiblioCommons', 'Library discovery platform with event listings.'),
]

# Patterns that identify our own scrapers (skip entirely).
# Our scrapers set PRODIDs like -//Org Name//domain.com// or -//Community Calendar//...
OUR_SCRAPERS = [
    r'Community Calendar//',
    r'//combined//',
    r'//\w[\w\s&;\'.\-]+//[\w.\-]+\.\w{2,}//',  # -//Name//domain.tld//
    r'//\w[\w\s&;\'.\-]+//Events//EN',            # -//Name//Events//EN
    r'//\w[\w\s&;\'.\-]+//events//EN',             # -//Name//events//EN
    r'Your Organization//NONSGML',                  # default scraper template
]


def classify_prodid(prodid: str) -> str | tuple[str, str]:
    """Return (platform_name, description), 'skip' for our scrapers, or 'unknown'."""
    # Check known platforms first (takes priority over scraper patterns)
    for pattern, name, desc in PLATFORM_MAP:
        if re.search(pattern, prodid):
            return name, desc
    # Then check if it's one of our scrapers
    for pattern in OUR_SCRAPERS:
        if re.search(pattern, prodid):
            return 'skip'
    return 'unknown'


def scan_cities(repo_root: Path) -> tuple[dict, dict]:
    """Scan all cities. Return (platforms, unclassified).

    platforms: {name: {desc, cities: {city: [files]}}}
    unclassified: {prodid: {city: [files]}}
    """
    cities_dir = repo_root / 'cities'
    platforms = {}
    unclassified = defaultdict(lambda: defaultdict(list))

    for city_dir in sorted(cities_dir.iterdir()):
        if not city_dir.is_dir():
            continue
        city = city_dir.name
        for ics_file in sorted(city_dir.glob('*.ics')):
            if ics_file.name == 'combined.ics':
                continue
            try:
                with open(ics_file, 'r', errors='replace') as f:
                    header = ''
                    for i, line in enumerate(f):
                        header += line
                        if i > 50:
                            break
            except Exception:
                continue

            match = re.search(r'PRODID[;:](.+)', header)
            if not match:
                continue
            prodid = match.group(1).strip()
            result = classify_prodid(prodid)

            if result == 'skip':
                continue
            elif result == 'unknown':
                unclassified[prodid][city].append(ics_file.stem)
            else:
                name, desc = result
                if name not in platforms:
                    platforms[name] = {'desc': desc, 'cities': defaultdict(list)}
                platforms[name]['cities'][city].append(ics_file.stem)

    return platforms, dict(unclassified)


def generate_markdown(platforms: dict, unclassified: dict) -> str:
    """Generate the docs/prodid.md content."""
    lines = [
        '# ICS Platforms (PRODID Report)',
        '',
        'Auto-generated by `scripts/prodid.py`. Shows the distinct ICS platforms',
        'used across all city feeds, identified by their PRODID header.',
        '',
    ]

    # Summary table
    lines.append('## Summary')
    lines.append('')
    lines.append('| Platform | Description | Cities | Feeds |')
    lines.append('|----------|-------------|--------|------:|')

    sorted_platforms = sorted(
        platforms.items(),
        key=lambda x: sum(len(files) for files in x[1]['cities'].values()),
        reverse=True
    )

    for name, info in sorted_platforms:
        cities = sorted(info['cities'].keys())
        total_feeds = sum(len(files) for files in info['cities'].values())
        city_list = ', '.join(cities)
        lines.append(f'| **{name}** | {info["desc"]} | {city_list} | {total_feeds} |')

    # Detail by platform
    lines.append('')
    lines.append('## Detail')
    lines.append('')

    for name, info in sorted_platforms:
        total_feeds = sum(len(files) for files in info['cities'].values())
        lines.append(f'### {name} ({total_feeds} feeds)')
        lines.append('')
        lines.append(f'{info["desc"]}')
        lines.append('')
        for city in sorted(info['cities'].keys()):
            files = info['cities'][city]
            lines.append(f'**{city}** ({len(files)}): {", ".join(sorted(files))}')
            lines.append('')

    # Unclassified section
    if unclassified:
        lines.append('## Unclassified')
        lines.append('')
        lines.append('PRODIDs not yet mapped to a platform. If these are third-party platforms,')
        lines.append('add a pattern to `PLATFORM_MAP` in `scripts/prodid.py`. If they are our')
        lines.append('own scrapers, add a pattern to `OUR_SCRAPERS`.')
        lines.append('')
        lines.append('| PRODID | City | Feeds |')
        lines.append('|--------|------|-------|')
        for prodid in sorted(unclassified.keys()):
            city_files = unclassified[prodid]
            for city in sorted(city_files.keys()):
                files = city_files[city]
                lines.append(f'| `{prodid}` | {city} | {", ".join(sorted(files))} |')
        lines.append('')

    return '\n'.join(lines)


def main():
    repo_root = Path(__file__).parent.parent
    platforms, unclassified = scan_cities(repo_root)
    md = generate_markdown(platforms, unclassified)

    out_path = repo_root / 'docs' / 'prodid.md'
    out_path.write_text(md)
    print(f'Wrote {out_path}')

    total_feeds = sum(
        sum(len(f) for f in p['cities'].values())
        for p in platforms.values()
    )
    print(f'{len(platforms)} platforms, {total_feeds} feeds across {len(set(c for p in platforms.values() for c in p["cities"]))} cities')

    if unclassified:
        total_unknown = sum(len(f) for files in unclassified.values() for f in files.values())
        print(f'{len(unclassified)} unclassified PRODIDs ({total_unknown} feeds) â€” see docs/prodid.md')


if __name__ == '__main__':
    main()
