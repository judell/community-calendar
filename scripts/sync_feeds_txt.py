#!/usr/bin/env python3
"""Regenerate cities/*/feeds.txt from the workflow YAML.

The GitHub Actions workflow is the source of truth for what sources exist.
feeds.txt is a human-readable documentation artifact. This script keeps
them in sync by parsing the workflow and writing feeds.txt for each city.

Usage:
    python scripts/sync_feeds_txt.py                  # all cities
    python scripts/sync_feeds_txt.py --city santarosa  # one city
    python scripts/sync_feeds_txt.py --dry-run         # preview
"""

import argparse
import re
from pathlib import Path

ROOT = Path(__file__).parent.parent
WORKFLOW_PATH = ROOT / ".github/workflows/generate-calendar.yml"

# Cities and their section headers in the workflow
CITY_SECTIONS = {
    'santarosa': 'Santa Rosa',
    'bloomington': 'Bloomington',
    'davis': 'Davis',
    'petaluma': 'Petaluma',
    'toronto': 'Toronto',
    'raleighdurham': 'Raleigh-Durham',
}


def extract_city_blocks(workflow_text: str) -> dict[str, str]:
    """Extract the full workflow text block for each city."""
    blocks = {}
    # Split on the city separator comments
    parts = re.split(r'# =+\n\s*# (.+?)\n\s*# =+', workflow_text)
    # parts alternates: [preamble, "Santa Rosa", block, "Bloomington", block, ...]
    for i in range(1, len(parts) - 1, 2):
        header = parts[i].strip()
        block = parts[i + 1]
        # Map header back to city slug
        for slug, name in CITY_SECTIONS.items():
            if name == header:
                blocks[slug] = block
                break
    return blocks


def extract_curl_urls(block: str) -> list[tuple[str, str]]:
    """Extract (url, output_filename) from curl commands."""
    results = []
    for match in re.finditer(r'curl\s+.*?"(https?://[^"]+)".*?-o\s+(\S+)', block):
        url = match.group(1)
        filename = match.group(2)
        results.append((url, filename))
    return results


def extract_scraper_commands(block: str, city: str) -> list[tuple[str, str]]:
    """Extract (scraper_command, output_path) from python scraper lines.

    Excludes pipeline scripts (combine_ics, ics_to_json, validate, report, prodid).
    """
    PIPELINE_SCRIPTS = {'combine_ics', 'ics_to_json', 'validate_pipeline', 'report', 'prodid'}
    results = []

    def is_pipeline_script(cmd: str) -> bool:
        return any(s in cmd for s in PIPELINE_SCRIPTS)

    for match in re.finditer(r'(python\s+\S+\.py\s+.*?)--output\s+(\S+)', block):
        command = match.group(1).strip()
        output = match.group(2)
        if not is_pipeline_script(command):
            results.append((command, output))
    # Also match -o shorthand
    for match in re.finditer(r'(python\s+\S+\.py\s+.*?)\s+-o\s+(\S+)', block):
        command = match.group(1).strip()
        output = match.group(2)
        if not is_pipeline_script(command) and not any(o == output for _, o in results):
            results.append((command, output))
    # Also match stdout redirect: python foo.py > output.ics
    for match in re.finditer(r'(python\s+\S+\.py)\s*>\s*(\S+\.ics)', block):
        command = match.group(1).strip()
        output = match.group(2)
        if not is_pipeline_script(command) and not any(o == output for _, o in results):
            results.append((command, output))
    return results


def categorize_url(url: str) -> str:
    """Return a category comment for a URL."""
    if 'meetup.com' in url:
        return 'Meetup'
    if 'tockify.com' in url:
        return 'Tockify'
    if 'calendar.google.com' in url:
        return 'Google Calendar'
    if 'membershipworks.com' in url:
        return 'MembershipWorks'
    if 'livewhale' in url or '/live/ical/' in url:
        return 'LiveWhale'
    if 'libcal.com' in url:
        return 'LibCal'
    if 'ical=1' in url or 'ical' in url.lower():
        return 'iCal feed'
    return 'Live feed'


def categorize_scraper(command: str) -> str:
    """Return a category comment for a scraper command."""
    if 'library_intercept' in command:
        return 'Library'
    if 'maxpreps' in command:
        return 'High school athletics'
    if 'growthzone' in command:
        return 'Chamber of Commerce'
    if 'legistar' in command:
        return 'Government meetings'
    if 'squarespace' in command:
        return 'Squarespace'
    return 'Scraper'


def generate_feeds_txt(city: str, block: str) -> str:
    """Generate feeds.txt content for a city from its workflow block."""
    curls = extract_curl_urls(block)
    scrapers = extract_scraper_commands(block, city)

    lines = [f"# {CITY_SECTIONS[city]} - source inventory",
             f"# Auto-generated by scripts/sync_feeds_txt.py from the workflow YAML.",
             f"# Source of truth: .github/workflows/generate-calendar.yml",
             ""]

    # Group scrapers by category
    if scrapers:
        lines.append("# --- Scrapers ---")
        by_cat: dict[str, list] = {}
        for command, output in scrapers:
            cat = categorize_scraper(command)
            by_cat.setdefault(cat, []).append((command, output))
        for cat, items in by_cat.items():
            lines.append(f"# {cat}")
            for command, output in items:
                lines.append(f"# cmd: {command}")
                lines.append(output)
            lines.append("")

    # Group curls by category
    if curls:
        lines.append("# --- Live feeds ---")
        by_cat: dict[str, list] = {}
        for url, filename in curls:
            cat = categorize_url(url)
            by_cat.setdefault(cat, []).append((url, filename))
        for cat, items in by_cat.items():
            lines.append(f"# {cat}")
            for url, filename in items:
                lines.append(url)
            lines.append("")

    return '\n'.join(lines).rstrip() + '\n'


def main():
    parser = argparse.ArgumentParser(description='Sync feeds.txt from workflow YAML')
    parser.add_argument('--city', help='Only sync one city')
    parser.add_argument('--dry-run', action='store_true', help='Print instead of writing')
    args = parser.parse_args()

    workflow_text = WORKFLOW_PATH.read_text()
    blocks = extract_city_blocks(workflow_text)

    cities = [args.city] if args.city else list(CITY_SECTIONS.keys())

    for city in cities:
        if city not in blocks:
            print(f"⚠️  No workflow block found for {city}")
            continue

        content = generate_feeds_txt(city, blocks[city])
        feeds_path = ROOT / f"cities/{city}/feeds.txt"

        if args.dry_run:
            print(f"\n{'='*60}")
            print(f"  {feeds_path}")
            print(f"{'='*60}")
            print(content)
        else:
            feeds_path.write_text(content)
            # Count sources
            source_count = sum(1 for line in content.splitlines()
                             if line.strip() and not line.startswith('#'))
            print(f"✅ {city}: wrote {feeds_path} ({source_count} sources)")


if __name__ == '__main__':
    main()
