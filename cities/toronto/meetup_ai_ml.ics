BEGIN:VCALENDAR
VERSION:2.0
PRODID:-//Meetup//Meetup Calendar 1.0//EN
CALSCALE:GREGORIAN
METHOD:PUBLISH
NAME:Toronto AI, Machine Learning and Computer Vision Meetup
X-WR-CALNAME:Toronto AI, Machine Learning and Computer Vision Meetup
BEGIN:VTIMEZONE
TZID:America/Toronto
TZURL:http://tzurl.org/zoneinfo-outlook/America/Toronto
X-LIC-LOCATION:America/Toronto
BEGIN:DAYLIGHT
TZOFFSETFROM:-0500
TZOFFSETTO:-0400
TZNAME:EDT
DTSTART:19700308T020000
RRULE:FREQ=YEARLY;BYMONTH=3;BYDAY=2SU
END:DAYLIGHT
BEGIN:STANDARD
TZOFFSETFROM:-0400
TZOFFSETTO:-0500
TZNAME:EST
DTSTART:19701101T020000
RRULE:FREQ=YEARLY;BYMONTH=11;BYDAY=1SU
END:STANDARD
END:VTIMEZONE
BEGIN:VEVENT
UID:event_313301469@meetup.com
SEQUENCE:1
DTSTAMP:20260226T023604Z
DTSTART;TZID=America/Toronto:20260226T120000
DTEND;TZID=America/Toronto:20260226T130000
SUMMARY:Feb 26 - Exploring Video Datasets with FiftyOne and Vision-Languag
 e Models
DESCRIPTION:Toronto AI\, Machine Learning and Computer Vision Meetup\nJoin
  [Harpreet Sahota](https://www.linkedin.com/in/harpreetsahota204/) for a v
 irtual workshop to learn how to use Facebook's Action100M dataset and Fift
 yOne to build an end-to-end workflow.\n\n**Date\, Time and Location**\n\nF
 eb 26\, 2026\n9am - 10am Pacific\nOnline. **[Register for the Zoom!](https
 ://voxel51.com/events/exploring-video-datasets-with-fiftyone-and-vision-la
 nguage-models-february-26-2026)**\n\nVideo is the hardest modality to work
  with. You're dealing with more data\, temporal complexity\, and annotatio
 n workflows that don't scale. This hands-on workshop tackles a practical q
 uestion: given a large video dataset\, how do you understand what's in it 
 without manually watching thousands of clips?\n\nIn this workshop you'll l
 earn how to:\n\n* **Navigate and explore** video data in the FiftyOne App\
 , filter samples\, and understand dataset structure\n* **Compute embedding
 s** with Qwen3-VL to enable semantic search\, zero-shot classification\, a
 nd clustering\n* **Generate descriptions and localize events** using visio
 n-language models like Qwen3-VL and Molmo2\n* **Visualize patterns** in yo
 ur data through embedding projections and the FiftyOne App\n* **Evaluate m
 odel outputs** against Action100M's hierarchical annotations to validate w
 hat the models actually capture\n\nBy the end of the session\, you'll have
  a reusable toolkit for understanding any video dataset at scale\, whether
  you're curating training data\, debugging model performance\, or explorin
 g a new domain.
URL;VALUE=URI:https://www.meetup.com/toronto-ai-machine-learning-data-scie
 nce/events/313301469/
STATUS:CONFIRMED
CREATED:20260211T001243Z
LAST-MODIFIED:20260211T001243Z
CLASS:PUBLIC
END:VEVENT
BEGIN:VEVENT
UID:event_312769331@meetup.com
SEQUENCE:1
DTSTAMP:20260226T023604Z
DTSTART;TZID=America/Toronto:20260305T120000
DTEND;TZID=America/Toronto:20260305T140000
SUMMARY:March 5 - AI\, ML and Computer Vision Meetup
DESCRIPTION:Toronto AI\, Machine Learning and Computer Vision Meetup\nJoin
  our virtual meetup to hear talks from experts on cutting-edge topics acro
 ss AI\, ML\, and computer vision.\n\n**Date and Location**\n\nMar 5\, 2026
 \n9 - 11 AM Pacific\nOnline. **[Register for the Zoom!](https://voxel51.co
 m/events/ai-ml-and-computer-vision-meetup-march-5-2026)**\n\n**MOSPA: Huma
 n Motion Generation Driven by Spatial Audio**\n\nEnabling virtual humans t
 o dynamically and realistically respond to diverse auditory stimuli remain
 s a key challenge in character animation\, demanding the integration of pe
 rceptual modeling and motion synthesis. Despite its significance\, this ta
 sk remains largely unexplored. Most previous works have primarily focused 
 on mapping modalities like speech\, audio\, and music to generate human mo
 tion. As of yet\, these models typically overlook the impact of spatial fe
 atures encoded in spatial audio signals on human motion.\n\nTo bridge this
  gap and enable high-quality modeling of human movements in response to sp
 atial audio\, we introduce the first comprehensive Spatial Audio-Driven Hu
 man Motion (SAM) dataset\, which contains diverse and high-quality spatial
  audio and motion data. For benchmarking\, we develop a simple yet effecti
 ve diffusion-based generative framework for human MOtion generation driven
  by SPatial Audio\, termed MOSPA\, which faithfully captures the relations
 hip between body motion and spatial audio through an effective fusion mech
 anism. Once trained\, MOSPA can generate diverse\, realistic human motions
  conditioned on varying spatial audio inputs. We perform a thorough invest
 igation of the proposed dataset and conduct extensive experiments for benc
 hmarking\, where our method achieves state-of-the-art performance on this 
 task.\n\n*About the Speaker*\n\n[Zhiyang (Frank) Dou](https://www.linkedin
 .com/in/zhiyang-frank-dou-0259111b3/) is a Ph.D. student at MIT CSAIL\, ad
 vised by Prof. Wojciech Matusik. I work with the Computational Design and 
 Fabrication Group and the Computer Graphics Group.\n\n**Securing the Auton
 omous Future: Navigating the Intersection of Agentic AI\, Connected Device
 s\, and Cyber Resilience**\n\nWith billions of devices now in our infrastr
 ucture and emerging as autonomous agents (AI)\, we face a very real questi
 on: How can we create intelligent systems that are both secure and trusted
 ? This talk will explore the intersection of agentic AI and IoT and demons
 trate how the same AI systems can provide robust defense mechanisms. At it
 s core\, however\, this is a challenge about trusting people with technolo
 gy\, ensuring their safety\, and providing accountability. Therefore\, cre
 ating a new way of thinking is required\, one in which security is built i
 n\, and where autonomous action has oversight\; and\, ultimately\, innovat
 ion leads to greater human well-being.\n\n*About the Speaker*\n\n[Samaresh
  Kumar Singh](https://www.linkedin.com/in/samaresh-singh-9772ba23/) is an 
 engineering principal at HP Inc. with more than 21 years of experience in 
 designing and implementing large-scale distributed systems\, cloud native 
 platform systems\, and edge AI / ML systems. His expertise includes agenti
 c AI systems\, GenAI / LLMs\, Edge AI\, federated and privacy preserving l
 earning\, and secure hybrid cloud / edge computing.\n\n**Plugins as Produc
 ts: Bringing Visual AI Research into Real-World Workflows with FiftyOne**\
 n\nVisual AI research often introduces new datasets\, models\, and analysi
 s methods\, but integrating these advances into everyday workflows can be 
 challenging. FiftyOne is a data-centric platform designed to help teams ex
 plore\, evaluate\, and improve visual AI\, and its plugin ecosystem is how
  the platform scales beyond the core. In this talk\, we explore the FiftyO
 ne plugin ecosystem from both perspectives: how users apply plugins to acc
 elerate data-centric workflows\, and how researchers and engineers can pac
 kage their work as plugins to make it easier to share\, reproduce\, and bu
 ild upon. Through practical examples\, we show how plugins turn research a
 rtifacts into reusable components that integrate naturally into real-world
  visual AI workflows.\n\n*About the Speaker*\n\n[Adonai Vera](https://www.
 linkedin.com/in/adonai-vera/) \\- Machine Learning Engineer & DevRel at Vo
 xel51\\. With over 7 years of experience building computer vision and mach
 ine learning models using TensorFlow\\\, Docker\\\, and OpenCV\\.\n\n**Tra
 nsforming Business with Agentic AI**\n\nAgentic AI is reshaping business o
 perations by employing autonomous systems that learn\, adapt\, and optimiz
 e processes independently of human input. This session examines the essent
 ial differences between traditional AI agents and Agentic AI\, emphasizing
  their significance for project professionals overseeing digital transform
 ation initiatives. Real-world examples from eCommerce\, insurance\, and he
 althcare illustrate how autonomous AI achieves measurable outcomes across 
 industries. The session addresses practical orchestration patterns in whic
 h specialized AI agents collaborate to resolve complex business challenges
  and enhance operational efficiency. Attendees will receive a practical fr
 amework for identifying high-impact use cases\, developing infrastructure\
 , establishing governance\, and scaling Agentic AI within their organizati
 ons.\n\n*About the Speaker*\n\n[Joyjit Roy](https://www.linkedin.com/in/ro
 yjoyjit/) is a senior technology and program management leader with over 2
 1 years of experience delivering enterprise digital transformation\, cloud
  modernization\, and applied AI programs across insurance\, financial serv
 ices\, and global eCommerce.
URL;VALUE=URI:https://www.meetup.com/toronto-ai-machine-learning-data-scie
 nce/events/312769331/
STATUS:CONFIRMED
CREATED:20260107T163938Z
LAST-MODIFIED:20260107T163938Z
CLASS:PUBLIC
END:VEVENT
BEGIN:VEVENT
UID:event_313022948@meetup.com
SEQUENCE:1
DTSTAMP:20260226T023604Z
DTSTART;TZID=America/Toronto:20260311T130000
DTEND;TZID=America/Toronto:20260311T140000
SUMMARY:March 11 - Strategies for Validating World Models and Action-Condi
 tioned Video
DESCRIPTION:Toronto AI\, Machine Learning and Computer Vision Meetup\nJoin
  us for a one hour hands-on workshop where we will explore emerging challe
 nges in developing and validating world foundation models and video-genera
 tion AI systems for robotics and autonomous vehicles.\n\n**Time and Locati
 on**\n\nMar 11\, 2026\n10-11am PST\nOnline\, **[Register for the Zoom!](ht
 tps://voxel51.com/events/debugging-the-future-strategies-for-validating-wo
 rld-models-and-action-conditioned-video-march)**\n\nIndustries from roboti
 cs to autonomous vehicles are converging on world foundation models (WFMs)
  and action-conditioned video generation\, where the challenge is predicti
 ng physics\, causality\, and intent. But this shift has created a massive 
 new bottleneck: validation.\n\nHow do you debug a model that imagines the 
 future? How do you curate petabyte-scale video datasets to capture the "lo
 ng tail" of rare events without drowning in storage costs? And how do you 
 ensure temporal consistency when your training data lives in scattered dat
 a lakes?\n\nIn this session\, we explore technical workflows for the next 
 generation of Visual AI. We will dissect the "Video Data Monster\," demons
 trating how to build feedback loops that bridge the gap between generative
  imagination and physical reality. Learn how leading teams are using feder
 ated data strategies and collaborative evaluation to turn video from a sto
 rage burden into a structured\, queryable asset for embodied intelligence.
 \n\n*About the Speaker*\n\n[Nick Lotz](https://www.linkedin.com/in/nichola
 slotz/) is chemical process engineer-turned-developer who is currently a T
 echnical Marketing Engineer at Voxel51. He is particularly interested in b
 ringing observability and security to all layers of the AI stack.
URL;VALUE=URI:https://www.meetup.com/toronto-ai-machine-learning-data-scie
 nce/events/313022948/
STATUS:CONFIRMED
CREATED:20260123T175746Z
LAST-MODIFIED:20260123T175746Z
CLASS:PUBLIC
END:VEVENT
BEGIN:VEVENT
UID:event_313284584@meetup.com
SEQUENCE:1
DTSTAMP:20260226T023604Z
DTSTART;TZID=America/Toronto:20260312T150000
DTEND;TZID=America/Toronto:20260312T170000
SUMMARY:March 12 - Agents\, MCP and Skills Virtual Meetup
DESCRIPTION:Toronto AI\, Machine Learning and Computer Vision Meetup\nJoin
  us for a special edition of the AI\, ML and Computer Vision Meetup where 
 we will focus on Agents\, MCP and Skills!\n\n**Date\, Time\, Location**\n\
 nMar 12\, 2026\n9 - 11 AM PST\nOnline. **[Register for the Zoom!](https://
 voxel51.com/events/mcp-and-skills-meetup-march-12-2026)**\n\n**Agents Buil
 ding Agents on the Hugging Face Hub**\n\nDiscover how coding agents can ru
 n or support your fine-tuning experiments. From quick dataset validation a
 nd preprocessing\, to optimal GPU hardware selection\, to automated job su
 bmission based on metric tracking\, to evaluation. Ben will demonstrate ho
 w Hugging Face skills can be used to define best practices for agents to s
 upport machine learning experiments. Bring Claude\, Codex\, or Mistral Vib
 es\, and we’ll show you to get it training models with GRPO\, SFT\, and 
 DPO.\n\n*About the Speaker*\n\n[Ben Burtenshaw](https://www.linkedin.com/i
 n/ben-burtenshaw/) is a Machine Learning Engineer at Hugging Face\, focusi
 ng on building agents with fine-tuning and reinforcement learning. He led 
 educational projects like the Agents Course\, the MCP Course\, and the LLM
  course\, which bridge the gap between complex Reinforcement Learning (RL)
  techniques and practical application. Ben focuses on democratizing access
  to efficient AI\, empowering the community to align\, evaluate\, and depl
 oy transparent agentic systems.\n\n**Claude Code Templates**\n\nThis talk 
 explores how to configure and align Claude Code agents using templates and
  custom components. I'll demonstrate practical configuration patterns that
  ensure your CLI agent executes exactly what you intend\, covering Skills 
 setup\, hooks implementation\, and template customization. Drawing from re
 al-world examples building Claude Code Templates\, attendees will learn ho
 w to structure their agent configurations for consistent\, reliable behavi
 or and create reusable components that maintain alignment across different
  use cases.\n\n*About the Speaker*\n\n[Daniel Avila](https://www.linkedin.
 com/in/daniel-avila-arias/) is an AI Engineer at Hedgineer building agenti
 c systems and creator of Claude Code Templates.\n\n**Move Faster in Comput
 er Vision by Teaching Agents to See Your Data**\n\nComputer vision teams s
 pend too much time writing scripts just to find bad labels\, blurry images
 \, and edge cases. In this talk\, I’ll show how to move that work to age
 nts by using FiftyOne as a visual operating system. With Skills and MCP\, 
 agents can see inside your datasets\, explore them visually\, and handle c
 ommon data cleanup tasks\, so you can spend less time on data and more tim
 e shipping models.\n\n*About the Speaker*\n\n[Adonai Vera](https://www.lin
 kedin.com/in/adonai-vera/) \\- Machine Learning Engineer & DevRel at Voxel
 51\\. With over 7 years of experience building computer vision and machine
  learning models using TensorFlow\\\, Docker\\\, and OpenCV\\. I started a
 s a software developer\\\, moved into AI\\\, led teams\\\, and served as C
 TO\\. Today\\\, I connect code and community to build open\\\, production\
 \-ready AI\\\, making technology simple\\\, accessible\\\, and reliable\\.
 \n\n**Skills As Documentation**\n\nSkills are self-contained recipes - eac
 h one a piece of a larger puzzle. Instead of trying to modify human-centri
 c documentation to better fit agents\, skills let us build capabilities in
 to our agents directly. This talk will showcase how to think about leverag
 ing skills to enhance how users interact with your software!\n\n*About the
  Speaker*\n\n[Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/) is a 
 deep learning developer advocate at NVIDIA\, working on creating technical
  assets that help developers use the incredible suite of AI tools availabl
 e at NVIDIA. Chris comes from a machine learning and data science backgrou
 nd\, and he is obsessed with everything and anything about large language 
 models.
URL;VALUE=URI:https://www.meetup.com/toronto-ai-machine-learning-data-scie
 nce/events/313284584/
STATUS:CONFIRMED
CREATED:20260209T201130Z
LAST-MODIFIED:20260209T201130Z
CLASS:PUBLIC
END:VEVENT
BEGIN:VEVENT
UID:event_313103257@meetup.com
SEQUENCE:1
DTSTAMP:20260226T023604Z
DTSTART;TZID=America/Toronto:20260318T120000
DTEND;TZID=America/Toronto:20260318T130000
SUMMARY:March 18 - Vibe Coding Production-Ready Computer Vision Pipelines 
 Workshop
DESCRIPTION:Toronto AI\, Machine Learning and Computer Vision Meetup\nJoin
  us for an interactive workshop where we'll build production-ready compute
 r vision pipelines using vibe coded [FiftyOne plugins.](https://docs.voxel
 51.com/plugins/index.html)\n\n**[Register for the Zoom](https://voxel51.co
 m/events/vibe-coding-production-ready-computer-vision-pipelines-hands-on-w
 orkshop-march-18-2026)**\n\nPlugins enable you to customize the open-sourc
 e [FiftyOne computer vision app](https://docs.voxel51.com/index.html) to m
 atch your exact workflow by easily incorporating data annotation\, curatio
 n\, model evaluation and inference.\n\nWe'll demonstrate how FiftyOne Skil
 ls and the MCP Server can streamline the journey from prototype to product
 ion-ready pipelines\, keeping your development flow intact.\n\nPerfect for
  open-source contributors\, researchers\, and enterprise teams seeking han
 ds-on experience. All participants receive slides\, notebooks\, and access
  to GitHub repositories and videos from the workshop.
URL;VALUE=URI:https://www.meetup.com/toronto-ai-machine-learning-data-scie
 nce/events/313103257/
STATUS:CONFIRMED
CREATED:20260128T233141Z
LAST-MODIFIED:20260128T233141Z
CLASS:PUBLIC
END:VEVENT
BEGIN:VEVENT
UID:event_312787364@meetup.com
SEQUENCE:1
DTSTAMP:20260226T023604Z
DTSTART;TZID=America/Toronto:20260319T120000
DTEND;TZID=America/Toronto:20260319T140000
SUMMARY:March 19 - Women in AI Meetup
DESCRIPTION:Toronto AI\, Machine Learning and Computer Vision Meetup\nHear
  talks from experts on the latest topics in AI\, ML\, and computer vision 
 on March 19th.\n\n**Date and Location**\n\nMar 19\, 2026\n9 - 11 AM Pacifi
 c\nOnline. **[Register for Zoom!](https://voxel51.com/events/women-in-ai-m
 eetup-march-19-2026)**\n\n**Towards Reliable Clinical AI: Evaluating Factu
 ality\, Robustness\, and Real-World Performance of Large Language Models**
 \n\nLarge language models are increasingly deployed in clinical settings\,
  but their reliability remains uncertain—they hallucinate facts\, behave
  inconsistently across instruction phrasings\, and struggle with evolving 
 medical terminology. In my talk\, I address methods to systematically eval
 uate clinical LLM reliability across four dimensions aligned with how heal
 thcare professionals actually work: verifying concrete facts (FactEHR)\, e
 nsuring stable guidance across instruction variations (instruction sensiti
 vity study showing up to 0.6 AUROC variation)\, integrating up-to-date kno
 wledge (BEACON improving biomedical NER by 15%)\, and assessing real patie
 nt conversations (PATIENT-EVAL revealing models abandon safety warnings wh
 en patients seek reassurance). These contributions establish evaluation st
 andards spanning factuality\, robustness\, knowledge integration\, and pat
 ient-centered communication\, charting a path toward clinical AI that is s
 afer\, more equitable\, and more trustworthy.\n\n*About the Speaker*\n\n[M
 onica Munnangi](https://www.linkedin.com/in/monicamunnangi/) is a doctoral
  student at the Khoury College of Computer Sciences at Northeastern Univer
 sity\, advised by Saiph Savage. Her doctoral research\, which she began in
  2021 and expects to complete in 2026\, focuses on multi-modal machine lea
 rning for healthcare. After being introduced to artificial intelligence an
 d machine learning during her undergraduate studies\, Munnangi earned her 
 master’s degree from UMass Amherst.\n\n**Neural BRDFs: Learning Compact 
 Representations for Material Appearance**\n\nAccurately modeling how light
  interacts with real-world materials remains a central challenge in render
 ing. Bidirectional Reflectance Distribution Functions (BRDFs) describe how
  materials reflect light as a function of viewing and lighting directions.
  Creating realistic digital materials has traditionally required choosing 
 between fast parametric models that can't capture real-world complexity\, 
 or massive measured BRDFs that are expensive to acquire and store. Neural 
 BRDFs address this challenge by learning continuous reflectance functions 
 from data\, exploiting directional correlations and symmetry to achieve si
 gnificant compression while maintaining rendering quality. In this talk\, 
 we examine how neural networks can encode complex material behavior compac
 tly\, why this matters for rendering and material capture\, and how neural
  BRDFs fit into the broader evolution toward data-driven graphics.\n\n*Abo
 ut the Speaker*\n\n[Manushree Gangwar](https://www.linkedin.com/in/manushr
 eegangwar/) is a Machine Learning Engineer at Voxel51 working on data-cent
 ric visual AI. She holds an MS in Computer Science from Columbia Universit
 y and has previously worked in robotics\, autonomous driving\, and AR/VR\,
  with a focus on scene understanding and 3D reconstruction.\n\n**Superchar
 ging AI agents with evaluations**\n\nReliable deployment of AI agents depe
 nds on rigorous evaluation\, which must shift from a nice-to-have QA step 
 to a core engineering discipline. Robust evaluation is essential for safet
 y\, predictability\, misuse resistance\, and sustained user trust. To meet
  this bar\, Evals must be deeply integrated into the agent development lif
 ecycle. This talk will outline how simulation-based testing—using high-f
 idelity\, controllable environments—provides the next generation of eval
 uation infrastructure for production-ready AI agents.\n\n*About the Speake
 r*\n\n[Priya Venkat](https://www.linkedin.com/in/priya26/)\, PhD\, is a Se
 nior AI Manager at Intuit\, where she leads teams that build and scale ML 
 and Agentic AI systems for finance. Her work integrates cutting-edge agent
 ic workflows and robust evaluation systems to drive business impact while 
 ensuring AI safety and reliability. Priya is a strong advocate of responsi
 ble AI\, and actively mentors the next generation of AI scientists and eng
 ineers.\n\n**Language Diffusion Models**\n\nAutoregressive models (ARMs) a
 re widely regarded as the cornerstone of large language models (LLMs). Cha
 llenge this notion by introducing LLaDA\, a diffusion model trained from s
 cratch under the pre-training and supervised fine-tuning (SFT) paradigm. L
 LaDA models distributions through a forward data masking process and a rev
 erse process\, parameterized by a vanilla Transformer to predict masked to
 kens. Optimizing a likelihood bound provides a principled generative appro
 ach for probabilistic inference. Across extensive benchmarks\, LLaDA demon
 strates strong scalability\, outperforming self-constructed ARM baselines.
  Remarkably\, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in i
 n-context learning and\, after SFT\, exhibits impressive instruction-follo
 wing abilities in case studies such as multi-turn dialogue.\n\n*About the 
 Speaker*\n\n[Jayita Bhattacharyya](https://www.linkedin.com/in/jayita-bhat
 tacharyya) is a AI ML Nerd with a blend of technical speaking & hackathon 
 wizardry! Applying tech to solve real-world problems. The work focus these
  days is on generative AI. Helping software teams incorporate AI into tran
 sforming software engineering.
URL;VALUE=URI:https://www.meetup.com/toronto-ai-machine-learning-data-scie
 nce/events/312787364/
STATUS:CONFIRMED
CREATED:20260108T171049Z
LAST-MODIFIED:20260108T171049Z
CLASS:PUBLIC
END:VEVENT
BEGIN:VEVENT
UID:event_313081597@meetup.com
SEQUENCE:1
DTSTAMP:20260226T023604Z
DTSTART;TZID=America/Toronto:20260402T120000
DTEND;TZID=America/Toronto:20260402T140000
SUMMARY:April 2 - AI\, ML and Computer Vision Meetup
DESCRIPTION:Toronto AI\, Machine Learning and Computer Vision Meetup\nJoin
  our virtual Meetup to hear talks from experts on cutting-edge topics acro
 ss AI\, ML\, and computer vision.\n\n**Date\, Time and Location**\n\nApr 2
 \, 2026\n9 - 11 AM Pacific\nOnline. **[Register for the Zoom!](https://vox
 el51.com/events/ai-ml-and-computer-vision-meetup-april-2-2026)**\n\n**Asyn
 c Agents in Production: Failure Modes and Fixes**\n\nAs models improve\, w
 e are starting to build long-running\, asynchronous agents such as deep re
 search agents and browser agents that can execute multi-step workflows aut
 onomously. These systems unlock new use cases\, but they fail in ways that
  short-lived agents do not.\n\nThe longer an agent runs\, the more early m
 istakes compound\, and the more token usage grows through extended reasoni
 ng\, retries\, and tool calls. Patterns that work for request-response age
 nts often break down\, leading to unreliable behaviour and unpredictable c
 osts.\n\nThis talk is aimed at use case developers\, with secondary releva
 nce for platform engineers. It covers the most common failure modes in asy
 nc agents and practical design patterns for reducing error compounding and
  keeping token costs bounded in production.\n\n*About the Speaker*\n\n[Mer
 yem Arik](https://www.linkedin.com/in/meryemarik/) is the co-founder and C
 EO of Doubleword\, where she works on large-scale LLM inference and produc
 tion AI systems. She studied theoretical physics and philosophy at the Uni
 versity of Oxford. Meryem is a frequent conference speaker\, including a T
 EDx speaker and a four-time highly rated speaker at QCon conferences. She 
 was named to the Forbes 30 Under 30 list for her work in AI infrastructure
 .\n\n**Visual AI at the Edge: Beyond the Model**\n\nEdge-based visual AI p
 romises low latency\, privacy\, and real-time decision-making\, but many p
 rojects struggle to move beyond successful demos. This talk explores what 
 deploying visual AI at the edge really involves\, shifting the focus from 
 models to complete\, operational systems. We will discuss common pitfalls 
 teams encounter when moving from lab to field. Attendees will leave with a
  practical mental model for approaching edge vision projects more effectiv
 ely.\n\n*About the Speaker*\n\n[David Moser](https://www.linkedin.com/in/d
 avid-moser-dm/) is an AI/Computer Vision expert and Founding Engineer with
  a strong track record of building and deploying safety-critical visual AI
  systems in real-world environments. As Co-Founder of Orella Vision\, he i
 s building Visual AI for Autonomy on the Edge - going from data and models
  to production-grade edge deployments.\n\n**Sanitizing Evaluation Datasets
 : From Detection to Correction**\n\nWe generally accept that gold standard
  evaluation sets contain label noise\, yet we rarely fix them because the 
 engineering friction is too high. This talk presents a workflow to operati
 onalize ground-truth auditing. We will demonstrate how to bridge the gap b
 etween algorithmic error detection and manual rectification. Specifically\
 , we will show how to inspect discordant ground truth labels and correct t
 hem directly in-situ. The goal is to move to a fully trusted end-to-end ev
 aluation pipeline.\n\n*About the Speaker*\n\n[Nick Lotz](https://www.linke
 din.com/in/nicholaslotz) is an engineer on the Voxel51 community team. Wit
 h a background in open source infrastructure and a passion for developer e
 nablement\, Nick focuses on helping teams understand their tools and how t
 o use them to ship faster.\n\n**Building enterprise agentic systems that s
 cale**\n\nBuilding AI agents that work in demos is easy\, building true as
 sistants that make people genuinely productive takes a different set of pa
 tterns. This talk shares lessons from a multi-agent system at Cisco used b
 y 2\,000+ sellers daily\, where we moved past "chat with your data" to enc
 oding business workflows into true agentic systems people actually rely on
  to get work done.\n\nWe'll cover multi-agent orchestration patterns for c
 omplex workflows\, the personalization and productivity features that driv
 e adoption\, and the enterprise foundations that helped us earn user trust
  at scale. You'll leave with an architecture and set of patterns that have
  been battle tested at enterprise scale.\n\n*About the Speaker*\n\n[Aman S
 ardana](https://www.linkedin.com/in/amansardana/) is a Senior Engineering 
 Architect at Cisco\, I lead the design and deployment of enterprise AI sys
 tems that blend LLMs\, data infrastructure\, and customer experience to so
 lve high‑stakes\, real-world problems at scale. I’m also an open-sourc
 e contributor and active mentor in the AI community\, helping teams move f
 rom AI experimentation to reliable agentic applications in production.
URL;VALUE=URI:https://www.meetup.com/toronto-ai-machine-learning-data-scie
 nce/events/313081597/
STATUS:CONFIRMED
CREATED:20260127T173221Z
LAST-MODIFIED:20260127T173221Z
CLASS:PUBLIC
END:VEVENT
BEGIN:VEVENT
UID:event_313298956@meetup.com
SEQUENCE:1
DTSTAMP:20260226T023604Z
DTSTART;TZID=America/Toronto:20260408T130000
DTEND;TZID=America/Toronto:20260408T140000
SUMMARY:April 8 - Getting Started with FiftyOne
DESCRIPTION:Toronto AI\, Machine Learning and Computer Vision Meetup\nThis
  workshop provides a technical foundation for managing large scale compute
 r vision datasets. You will learn to curate\, visualize\, and evaluate mod
 els using the [open source FiftyOne app](https://docs.voxel51.com/).\n\n**
 Date\, Time and Location**\n\nApr 8\, 2026\n10 AM PST - 11 AM Pacific\nOnl
 ine. **[Register for the Zoom!](https://voxel51.com/events/getting-started
 -with-fiftyone-april-8-2026)**\n\nThe session covers data ingestion\, embe
 dding visualization\, and model failure analysis. You will build workflows
  to identify dataset bias\, find annotation errors\, and select informativ
 e samples for training. Attendees leave with a framework for data centric 
 AI for research and production pipelines\, prioritizing data quality over 
 pure model iteration.\n\nWhat you'll learn\n\n* **Structure unstructured d
 ata.** Map data and metadata into a queryable schema for images\, videos\,
  and point clouds.\n* **Query datasets with the FiftyOne SDK**. Create com
 plex views based on model predictions\, labels\, and custom tags. Use the 
 FiftyOne to filter data based on logical conditions and confidence scores.
 \n* **Visualize high dimensional embeddings**. Project features into lower
  dimensions to find clusters of similar samples. Identify data gaps and ou
 tliers using FiftyOne Brain.\n* **Automate data curation.** Implement algo
 rithmic measures to select diverse subsets for training. Reduce labeling c
 osts by prioritizing high entropy samples.\n* **Debug model performance.**
  Run evaluation routines to generate confusion matrices and precision reca
 ll curves. Visualize false positives and false negatives directly in the A
 pp to understand model failures.\n* **Customize FiftyOne**. Build custom d
 ashboards and interactive panels. Create specialized views for domain spec
 ific tasks.\n\n**Prerequisites:**\n\n* Working knowledge of Python and mac
 hine learning and/or computer vision fundamentals.\n* All attendees will g
 et access to the tutorials and code examples used in the workshop.
URL;VALUE=URI:https://www.meetup.com/toronto-ai-machine-learning-data-scie
 nce/events/313298956/
STATUS:CONFIRMED
CREATED:20260210T203213Z
LAST-MODIFIED:20260210T203213Z
CLASS:PUBLIC
END:VEVENT
BEGIN:VEVENT
UID:event_313207137@meetup.com
SEQUENCE:1
DTSTAMP:20260226T023604Z
DTSTART;TZID=America/Toronto:20260409T120000
DTEND;TZID=America/Toronto:20260409T133000
SUMMARY:April 9 - Workshop: Build a Visual Agent that can Navigate GUIs li
 ke Humans
DESCRIPTION:Toronto AI\, Machine Learning and Computer Vision Meetup\nThis
  hands-on workshop provides a comprehensive introduction to building and e
 valuating visual agents for GUI automation using modern tools and techniqu
 es.\n\n**Date\, Time and Location**\n\nApril 9\, 2026 at 9 AM Pacific\nOnl
 ine. **[Register for the Zoom](https://voxel51.com/events/visual-agents-wh
 at-it-takes-to-build-an-agent-that-can-navigate-guis-like-humans-april-9-2
 026)**\n\nVisual agents that can understand and interact with graphical us
 er interfaces represent a transformative frontier in AI automation. These 
 systems combine computer vision\, natural language understanding\, and spa
 tial reasoning to enable machines to navigate complex interfaces—from we
 b applications to desktop software—just as humans do. However\, building
  robust GUI agents requires careful attention to dataset curation\, model 
 evaluation\, and iterative improvement workflows.\n\nParticipants will lea
 rn how to leverage [FiftyOne](https://github.com/voxel51/fiftyone)\, an op
 en-source toolkit for dataset curation and computer vision workflows\, to 
 build production-ready GUI agent systems.\n\nWhat You'll Learn:\n\n* **Dat
 aset Creation & Management:** How to structure\, annotate\, and load GUI i
 nteraction datasets using the COCO4GUI standardized format\n* **Data Explo
 ration & Analysis:** Using FiftyOne's interactive interface to visualize d
 atasets\, analyze action distributions\, and understand annotation pattern
 s\n* **Multimodal Embeddings:** Computing embeddings for screenshots and U
 I element patches to enable similarity search and retrieval\n* **Model Inf
 erence:** Running state-of-the-art models like Microsoft's GUI-Actor to pr
 edict interaction points from natural language instructions\n* **Performan
 ce Evaluation:** Measuring model accuracy using standard metrics and norma
 lized click distance to assess localization precision\n* **Failure Analysi
 s:** Investigating model failures through attention maps\, error pattern a
 nalysis\, and systematic debugging workflows\n* **Data-Driven Improvement:
 ** Tagging samples based on error types (attention misalignment vs. locali
 zation errors) to prioritize fine-tuning efforts\n* **Synthetic Data Gener
 ation:** Using FiftyOne plugins to augment training data with synthetic ta
 sk descriptions and variations\n\n*About the Speaker*\n\n[Harpreet Sahota]
 (https://www.linkedin.com/in/harpreetsahota204/) is a hacker-in-residence 
 and machine learning engineer with a passion for deep learning and generat
 ive AI. He’s got a deep interest in RAG\, Agents\, and Multimodal AI.
URL;VALUE=URI:https://www.meetup.com/toronto-ai-machine-learning-data-scie
 nce/events/313207137/
STATUS:CONFIRMED
CREATED:20260204T174317Z
LAST-MODIFIED:20260204T174317Z
CLASS:PUBLIC
END:VEVENT
BEGIN:VEVENT
UID:event_313313274@meetup.com
SEQUENCE:1
DTSTAMP:20260226T023604Z
DTSTART;TZID=America/Toronto:20260506T130000
DTEND;TZID=America/Toronto:20260506T140000
SUMMARY:May 6 - Building Composable Computer Vision Workflows in FiftyOne
DESCRIPTION:Toronto AI\, Machine Learning and Computer Vision Meetup\nThis
  workshop explores the [FiftyOne](https://docs.voxel51.com/) plugin framew
 ork to build custom computer vision applications. You’ll learn to extend
  the open source FiftyOne App with Python based panels and server side ope
 rators\, as well as integrate external tools for labeling\, vector search\
 , and model inference into your dataset views.\n\n**Date\, Time and Locati
 on**\n\nMay 6\, 2026\n10 AM - 11 AM PST\nOnline. **[Register for the Zoom!
 ](https://voxel51.com/events/building-composable-vision-workflows-in-fifty
 one-may-6-2026)**\n\n**What You'll Learn**\n\n* **Build Python plugins.** 
 Define plugin manifests and directory structures to register custom functi
 onality within the FiftyOne ecosystem.\n* **Develop server side operators.
 ** Write functions to execute model inference\, data cleaning\, or metadat
 a updates from the App interface.\n* **Build interactive panels.** Create 
 custom UI dashboards using to visualize model metrics or specialized datas
 et distributions.\n* **Manage operator execution contexts.** Pass data bet
 ween the App front end and your backend to build dynamic user workflows.\n
 * **Implement delegated execution.** Configure background workers to handl
 e long running data processing tasks without blocking the user interface.\
 n* **Build labeling integrations.** Streamline the flow of data between Fi
 ftyOne and annotation platforms through custom triggers and ingestion scri
 pts.\n* **Extend vector database support**. Program custom connectors for 
 external vector stores to enable semantic search across large sample datas
 ets.\n* **Package and share plugins**. Distribute your extensions internal
 ly and externally
URL;VALUE=URI:https://www.meetup.com/toronto-ai-machine-learning-data-scie
 nce/events/313313274/
STATUS:CONFIRMED
CREATED:20260211T192322Z
LAST-MODIFIED:20260211T192322Z
CLASS:PUBLIC
END:VEVENT
X-ORIGINAL-URL:https://www.meetup.com/toronto-ai-machine-learning-data-sci
 ence/events/ical/
X-WR-CALNAME:Toronto AI\, Machine Learning and Computer Vision Meetup
END:VCALENDAR